{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbaspour - 610398147 - Final Project (Image Captioning)\n",
    "The problem is import raw image for classification task. This also can be done by means of pretrained transfer-based ResNet18 model and LSTM neural network.\n",
    "First some important methods from practical libraries are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from matplotlib.image import imread\n",
    "from matplotlib.pyplot import figure, imshow, ion, plot, savefig, show, title, xlabel, ylabel\n",
    "from os import path\n",
    "from pandas import read_csv\n",
    "from PIL import Image\n",
    "from spacy import load\n",
    "from torch import cat, device, no_grad, tensor\n",
    "from torch.cuda import is_available\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Embedding, Linear, LSTM, Module, Sequential\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadings & Initializations\n",
    "Dataset directories and other variables are imported and defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and variable loadings\n",
    "spacyEng, Device, image_dir, captions_dir, SOS, PAD, UNK, EOS, Epochs, Divider, listLosses = load(\"en_core_web_sm\"), device(\"cuda\" if is_available() else \"cpu\"), \"Images\", \"Captions.txt\", \"<SOS>\", \"<PAD>\", \"<UNK>\", \"<EOS>\", range(20), 2000, []\n",
    "df = read_csv(captions_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing an Image with its Corresponding Captions from Dataset\n",
    "An image is shown with 5 captions describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageIndex = 5\n",
    "imshow(imread(image_dir + \"/\" + df.iloc[imageIndex, 0]))\n",
    "show()\n",
    "for Caption in range(imageIndex, imageIndex + 5):\n",
    "    print(f\"Caption - {df.iloc[Caption, 1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Construction\n",
    "One way build a vocabulary with own dataset (sentence of 'Captions.txt' folder), th sentence must be preprocessed and tokenized and make a list of preprocessed sentences called 'Sentences'. A word must appear in the list wether it exists in whole sentences of dataset higher than the value of an input threshold called 'Threshold'. 'PAD', starting of any sentence 'SOS', terminator of any sentence 'EOS' and 'UNK' are indexed in main vocabulary.\\\n",
    "Testing phase is also done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, Threshold):\n",
    "        self.itos, self.threshold = {0: PAD, 1: SOS, 2: EOS, 3: UNK}, Threshold\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(Text):\n",
    "        return [Token.text.lower() for Token in spacyEng.tokenizer(Text)]\n",
    "\n",
    "    def build_vocab(self, Sentences):\n",
    "        Frequencies, Index = Counter(), 4\n",
    "        for Sentence in Sentences:\n",
    "            for Word in self.tokenize(Sentence):\n",
    "                Frequencies[Word] += 1\n",
    "                \n",
    "                if Frequencies[Word] == self.threshold:\n",
    "                    self.stoi[Word], self.itos[Index] = Index, Word\n",
    "                    Index += 1\n",
    "\n",
    "    def numericalize(self, Text):\n",
    "        tokenizedText = self.tokenize(Text)\n",
    "        return [self.stoi[Token] if Token in self.stoi else self.stoi[UNK] for Token in tokenizedText]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding\n",
    "Another way to build this is to train Word2Vec embedding with own dataset (implementation is done in 'WordVec' defined function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way to construct the vocabulary using Word2Vec model\n",
    "def WordVec(Sentences):\n",
    "    validResponses = [\"CBOW\", \"SG\"]\n",
    "    while True:\n",
    "        modelMethod = input(\"CBOW or Skip-gram? <CBOW, Sg> \").upper()\n",
    "        if modelMethod in validResponses:\n",
    "            print(\"Alright!\")\n",
    "            break\n",
    "        print(\"Select one, dude!\")\n",
    "\n",
    "    if modelMethod == \"CBOW\":\n",
    "        # Create CBOW model\n",
    "        Model = Word2Vec(min_count = 5, vector_size = 300, window = 5)\n",
    "    else:\n",
    "        # Create Skip-gram model (sg = 1)\n",
    "        Model = Word2Vec(min_count = 5, sg = 1, vector_size = 300, window = 5)\n",
    "\n",
    "    Model.build_vocab(Sentences, progress_per = 1000)\n",
    "\n",
    "    # train on own data\n",
    "    Model.train(Sentences, epochs = 100, total_examples = len(Sentences))\n",
    "\n",
    "    return Model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesing Vocabulary Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \"Vocabulary\" class\n",
    "Voc = Vocabulary(Threshold = 1)\n",
    "Voc.build_vocab([\"Why this semester doesn't end?\", \"Oops!\", \"C'mon dude I'm over...\"])\n",
    "print(f\"\\n{Voc.stoi}\")\n",
    "print(Voc.numericalize(\"When does this semester end I'm exhausted dude!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Data prepration part by using constructed vocabulary (in any of two mentioned way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetCustomizer(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, Transform = None, Threshold = 5):\n",
    "        self.df, self.root_dir = read_csv(captions_file), root_dir\n",
    "\n",
    "        self.imgs, self.captions, self.transform = self.df[\"image\"], self.df[\"caption\"], Transform\n",
    "        \n",
    "        self.vocab = Vocabulary(Threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "        \n",
    "        # Word2Vec approach\n",
    "        #self.vocab = WordVec(self.captions.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, Index):\n",
    "        image, Caption = Image.open(path.join(self.root_dir, self.imgs[Index])).convert(\"RGB\"), self.captions[Index]\n",
    "\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "        return image, tensor([self.vocab.stoi[SOS]] + self.vocab.numericalize(Caption) + [self.vocab.stoi[EOS]])\n",
    "\n",
    "class CapsCollate:\n",
    "    def __init__(self, pad_idx, batchFirst = False):\n",
    "        self.batch_first, self.pad_idx = batchFirst, pad_idx\n",
    "        \n",
    "    def __call__(self, Batch):\n",
    "        return cat([Item[0].unsqueeze(0) for Item in Batch], dim = 0), pad_sequence([Item[1] for Item in Batch], batch_first = self.batch_first, padding_value = self.pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Neural Network Construction\n",
    "The Neural Network needed has two parts:\\\n",
    "Encoder which has pretrained ResNet18 model to act as feature extractor (freezing all layers except fully connected one, once, and making all layers and weights trainable for the second time) passes feature vectors with embedded word vectors (provided by Embedding layer). Decoder part has one LSTM recurrent network and a fully connected layer.\n",
    "Testing part which takes image and generates (hopefully) related captions is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, embeddingSize):\n",
    "        super(Encoder, self).__init__()\n",
    "        ResNet18 = resnet18(weights = True)\n",
    "        #ResNet18.load_state_dict(load(\"ResNet18 Pretrained Weights.pth\"))\n",
    "        ResNet18.eval()\n",
    "\n",
    "        # Freezing all layers and weights except the fully connected layer.\n",
    "        for Parameter in ResNet18.parameters():\n",
    "            Parameter.requires_grad = False\n",
    "\n",
    "        for Parameter in ResNet18.fc.parameters():\n",
    "            Parameter.requires_grad = True\n",
    "\n",
    "        # Unreezing all layers and weights.\n",
    "        \"\"\"\n",
    "        for Parameter in ResNet18.parameters():\n",
    "            Parameter.requires_grad = True\n",
    "        \"\"\"\n",
    "        summary(ResNet18, input_size = (3, 64, 64), batch_size = -1)\n",
    "        self.resnet = Sequential(*list(ResNet18.children())[:-1])\n",
    "        self.embedding = Linear(ResNet18.fc.in_features, embeddingSize)\n",
    "\n",
    "    def forward(self, Images):\n",
    "        Features = self.resnet(Images)\n",
    "        return self.embedding(Features.view(Features.size(0), -1))\n",
    "\n",
    "class Decoder(Module):\n",
    "    def __init__(self, embeddingSize, hiddenSize, vocabularySize, numberLayers = 1, dropProbability = 0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding, self.lstm, self.fcn, self.drop = Embedding(vocabularySize, embeddingSize), LSTM(embeddingSize, hiddenSize, num_layers = numberLayers, batch_first = True), Linear(hiddenSize, vocabularySize), Dropout(dropProbability)\n",
    "\n",
    "    def forward(self, Features, captions):\n",
    "        x, _ = self.lstm(cat((Features.unsqueeze(1), self.embedding(captions[:,:-1])), dim = 1))\n",
    "        return self.fcn(x)\n",
    "\n",
    "    # Testing time!\n",
    "    def captionGenerator(self, Inputs, Hidden = None, maximumLength = 20, Vocab = None):\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "        captions = []\n",
    "        \n",
    "        for i in range(maximumLength):\n",
    "            Output, Hidden = self.lstm(Inputs, Hidden)\n",
    "            Output = self.fcn(Output).view(Inputs.size(0), -1)\n",
    "            \n",
    "            #select the word with most val\n",
    "            predictedIndex = Output.argmax(dim = 1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predictedIndex.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if Vocab.itos[predictedIndex.item()] == EOS:\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            Inputs = self.embedding(predictedIndex.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [Vocab.itos[Index] for Index in captions]\n",
    "\n",
    "class encoderDecoder(Module):\n",
    "    def __init__(self, embeddingSize, hiddenSize, vocabularySize, numberLayers = 1, dropProbability = 0.3):\n",
    "        super(encoderDecoder, self).__init__()\n",
    "        self.encoder, self.decoder = Encoder(embeddingSize), Decoder(embeddingSize, hiddenSize, vocabularySize, numberLayers, dropProbability)\n",
    "\n",
    "    def forward(self, images, captions): \n",
    "        return self.decoder(self.encoder(images), captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating Images\n",
    "A function is implemented for illustrating pictures with a related title (caption) generated by model among epochs while training process to meassure how well model's perfomance is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transform to be applied\n",
    "def Illustrator(Input, Title = None):\n",
    "    imshow(Input.numpy().transpose((1, 2, 0)))\n",
    "    if Title != None:\n",
    "        title(Title)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Starter Pack\n",
    "Making a data loader format of (image, caption) tuple for being iterated while training.\\\n",
    "Model architecture, crossentropy loss and Adam optimizer are also initialized.\\\n",
    "Embedding vector length and the number of hidden are 300 and 256, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.0' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Sepehr/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Dataset construction\n",
    "Dataset = datasetCustomizer(root_dir = image_dir, captions_file = captions_dir, Transform = Compose([Resize((224, 224)), ToTensor()]))\n",
    "vocab = Dataset.vocab\n",
    "PI, vocabularySize = vocab.stoi[PAD], len(vocab)\n",
    "Dataset = DataLoader(dataset = Dataset, batch_size = 4, num_workers = 1, shuffle = True, collate_fn = CapsCollate(pad_idx = PI, batchFirst = True), pin_memory = True)\n",
    "\n",
    "# Initializing model, loss and optimizer\n",
    "Model, loss, datasetSize = encoderDecoder(300, 256, vocabularySize, 2).to(Device), CrossEntropyLoss(ignore_index = PI), len(Dataset)\n",
    "Optimizer = Adam(Model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase\n",
    "It's training time!\\\n",
    "Testing model during being trained is done after 'printEvery' times of diagnosing (images, caption) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.0' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Sepehr/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "for Epoch in Epochs:\n",
    "    Losses = 0\n",
    "    for Index, (image, captions) in enumerate(iter(Dataset)):\n",
    "        image, captions = image.to(Device), captions.to(Device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        Optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        Outputs = Model(image, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        Loss = loss(Outputs.view(-1, vocabularySize), captions.view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        Loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        Optimizer.step()\n",
    "        \n",
    "        Loss = Loss.item()\n",
    "        Losses += Loss\n",
    "\n",
    "        if (Index + 1) % Divider == 0:\n",
    "            print(\"In Epoch {} with loss {:.5f}\".format(Epoch + 1, Loss))\n",
    "            Model.eval()\n",
    "            with no_grad():\n",
    "                dataIter = iter(Dataset)\n",
    "                image, _ = next(dataIter)\n",
    "                Features = Model.encoder(image[0: 1].to(Device))\n",
    "                Illustrator(image[0], Title = \" \".join(Model.decoder.captionGenerator(Features.unsqueeze(0), Vocab = vocab)))\n",
    "           \n",
    "            # (FINALLY) training part!\n",
    "            Model.train()\n",
    "\n",
    "    listLosses.append(Losses / datasetSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Train Loss Trend\n",
    "What happens to mean train loss per epochs is plotted by following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal loss is {Loss}\") \n",
    "# Plotting the training history (losses)\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(Epochs, listLosses, \"red\")\n",
    "title(\"Training Evaluation\")\n",
    "ylabel(\"Mean Cross-entropy Train Loss\")\n",
    "xlabel(\"Epochs\")\n",
    "savefig(Title, dpi = DPI)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Number of epochs is set to twenty due to time and hardware limitation. After all, during being trained in this number of epochs, both models was learning well and mean loss was on a getting decreased way. Model with fully trainable layers had better loss deduction than the model with pretrained layers and weights except for fully connected one. According to illustrations from 'Evaluations' folder, the better and more relevant caption generated by model for any image from dataset, causes the lower loss (usually less than or equals 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "Main challenge was to learn about 'PyTorch' library and way of model constructions and trainings.\\\n",
    "Importing pretrained ResNet18 was a big challenge while I had tried many times to import pretrained weights of the model for putting them in their corresponding layers in ResNet18 Keras architecture (which is defined and put in 'Unseccessful Modeling.py' file.)\\\n",
    "According to having these challenges, I've searched for PyTorch modeling as much as possible in any legal accessible source (especially using helpings of https://discuss.pytorch.org/u/ptrblck/summary) and I'd rather to use Kaggle GPU for the first time which may cause some similarities between written code and instructions with other guys'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
